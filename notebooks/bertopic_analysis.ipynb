{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ab25ca2-c218-4d1d-89f0-1bfe8b3088d4",
   "metadata": {},
   "source": [
    "<span style=\"color:blue; font-size:24px;\"><b>BERTopic analysis of university students complaints</b></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a46c648-c1e8-4622-b8a7-7769c286ca1b",
   "metadata": {},
   "source": [
    "<span style=\"color:orange; font-size:18px;\"><b>0. Loading the data</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e657702a-09da-4b5d-b5c5-77300051bed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1005 docs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Project root path\n",
    "BASE_DIR = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(BASE_DIR)\n",
    "\n",
    "DATA_PATH = os.path.join(BASE_DIR, \"data\", \"Datasetprojpowerbi.csv\")\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "texts = df[\"Reports\"].tolist()\n",
    "print(f\"Loaded {len(texts)} docs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2f6298-b856-44ca-8e34-c7b874eec2df",
   "metadata": {},
   "source": [
    "<span style=\"color:orange; font-size:18px;\"><b>1. Cleaning and tokenizing</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01b13119-aa0f-45c9-b0d1-077b7cf0ebcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned and tokenized 1005 docs.\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessing.clean_text import normalize_texts, lemmatize_and_tokenize\n",
    "\n",
    "# 1. Normalization\n",
    "cleaned_texts = normalize_texts(texts)\n",
    "\n",
    "# 2. Lemmatization and tokenization\n",
    "tokenized_texts = lemmatize_and_tokenize(cleaned_texts)\n",
    "join_texts = [\" \".join(words) for words in tokenized_texts]\n",
    "\n",
    "print(f\"Cleaned and tokenized {len(tokenized_texts)} docs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2f8adf-3400-4646-906f-b3ae9842f37a",
   "metadata": {},
   "source": [
    "<span style=\"color:orange; font-size:18px;\"><b>2. Training BERTopic models for diff topic number and calculating c_v and WETC</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39d3ab58-9b2e-453d-ae3c-62136f44b6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num topics: 5, C_v: 0.475,  WETC: 0.651, Average: 0.598\n",
      "Num topics: 6, C_v: 0.556,  WETC: 0.637, Average: 0.613\n",
      "Num topics: 7, C_v: 0.564,  WETC: 0.638, Average: 0.616\n",
      "Num topics: 8, C_v: 0.593,  WETC: 0.635, Average: 0.623\n",
      "Num topics: 9, C_v: 0.570,  WETC: 0.643, Average: 0.621\n",
      "Num topics: 10, C_v: 0.577,  WETC: 0.642, Average: 0.622\n",
      "Num topics: 11, C_v: 0.602,  WETC: 0.638, Average: 0.627\n",
      "Num topics: 12, C_v: 0.626,  WETC: 0.641, Average: 0.636\n",
      "Num topics: 13, C_v: 0.631,  WETC: 0.637, Average: 0.635\n",
      "Num topics: 14, C_v: 0.659,  WETC: 0.638, Average: 0.645\n",
      "Num topics: 15, C_v: 0.664,  WETC: 0.641, Average: 0.648\n",
      "Num topics: 16, C_v: 0.669,  WETC: 0.641, Average: 0.649\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "from src.vectorization.vecorize_lsa_lda import build_dictionary\n",
    "from src.topic_models.bertopic_model import train_bertopic\n",
    "from src.metrics.wetc import wetc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "dictionary = build_dictionary(tokenized_texts)\n",
    "\n",
    "for num_topics in range(5, 17):\n",
    "    bert_model, topics = train_bertopic(\n",
    "        join_texts, \n",
    "        nr_topics=num_topics, \n",
    "        verbose=False, \n",
    "        min_topic_size=7,\n",
    "        vectorizer=vectorizer,\n",
    "        top_n_words=10\n",
    "    )\n",
    "    \n",
    "    topics_list = [[w for w,_ in words] for tid, words in bert_model.get_topics().items() if tid!=-1]\n",
    "    \n",
    "    coherence_model = CoherenceModel(\n",
    "        topics=topics_list,\n",
    "        texts=tokenized_texts,   \n",
    "        dictionary=dictionary,\n",
    "        coherence='c_v',\n",
    "        window_size=10\n",
    "    )\n",
    "    cv_score = coherence_model.get_coherence()\n",
    "    wetc_score = wetc(topics_list)\n",
    "    waver = 0.7 * wetc_score + 0.3 * cv_score\n",
    "    print(f\"Num topics: {num_topics}, C_v: {cv_score:.3f},  WETC: {wetc_score:.3f}, Average: {waver:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6729c350-1952-48ee-825d-20325c570155",
   "metadata": {},
   "source": [
    "<span style=\"color:orange; font-size:18px;\"><b>3. Topics statistics for num_topics=16 (15 topics + 1 outliers)</b></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fe8930f-954b-4e52-beae-887bdeb33ee6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Top Words</th>\n",
       "      <th>Documents (max prob)</th>\n",
       "      <th>Documents (second max prob)</th>\n",
       "      <th>Representative Document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>course, online, class, material, availability,...</td>\n",
       "      <td>174</td>\n",
       "      <td>479</td>\n",
       "      <td>I'm having trouble finding the course material...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>food, option, cafeteria, campus, offer, cantin...</td>\n",
       "      <td>141</td>\n",
       "      <td>226</td>\n",
       "      <td>6. \"The pizza available in the cantine is terr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>academic, balance, stress, workload, time, wor...</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>The academic workload is becoming overwhelming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>job, opportunity, internship, field, career, p...</td>\n",
       "      <td>97</td>\n",
       "      <td>41</td>\n",
       "      <td>There seems to be a scarcity of available inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>international, language, cultural, sometimes, ...</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "      <td>10. \"I've had some negative experiences with o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>sport, athletic, athlete, team, eligibility, g...</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>I'm disappointed with the gender inequality in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>financial, aid, loan, scholarship, pay, tuitio...</td>\n",
       "      <td>74</td>\n",
       "      <td>28</td>\n",
       "      <td>The financial aid process is so slow, and I n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>research, access, database, software, technolo...</td>\n",
       "      <td>49</td>\n",
       "      <td>140</td>\n",
       "      <td>It's frustrating to have limited access to res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>housing, rent, affordable, live, near, apartme...</td>\n",
       "      <td>37</td>\n",
       "      <td>68</td>\n",
       "      <td>9. \"The housing options near campus are very l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>medical, expense, insurance, treatment, pay, c...</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>It's frustrating that my health insurance doe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>bus, transportation, public, near, parking, ho...</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>2. \"I've had to commute from the suburbs to th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>certificate, affair, office, process, birth, s...</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The student affairs office seems to be disorg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>mental, health, care, provider, affordable, af...</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>I wish the university offered better mental he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>trip, explore, great, new, love, activity, out...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>2. \"I'm really interested in trying out some n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>advisor, academic, access, plan, career, limit...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>It's frustrating to have limited access to aca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic                                          Top Words  \\\n",
       "3       3  course, online, class, material, availability,...   \n",
       "0       0  food, option, cafeteria, campus, offer, cantin...   \n",
       "2       2  academic, balance, stress, workload, time, wor...   \n",
       "1       1  job, opportunity, internship, field, career, p...   \n",
       "4       4  international, language, cultural, sometimes, ...   \n",
       "5       5  sport, athletic, athlete, team, eligibility, g...   \n",
       "6       6  financial, aid, loan, scholarship, pay, tuitio...   \n",
       "7       7  research, access, database, software, technolo...   \n",
       "10     10  housing, rent, affordable, live, near, apartme...   \n",
       "8       8  medical, expense, insurance, treatment, pay, c...   \n",
       "9       9  bus, transportation, public, near, parking, ho...   \n",
       "11     11  certificate, affair, office, process, birth, s...   \n",
       "12     12  mental, health, care, provider, affordable, af...   \n",
       "14     14  trip, explore, great, new, love, activity, out...   \n",
       "13     13  advisor, academic, access, plan, career, limit...   \n",
       "\n",
       "    Documents (max prob)  Documents (second max prob)  \\\n",
       "3                    174                          479   \n",
       "0                    141                          226   \n",
       "2                    100                            2   \n",
       "1                     97                           41   \n",
       "4                     92                            2   \n",
       "5                     86                            0   \n",
       "6                     74                           28   \n",
       "7                     49                          140   \n",
       "10                    37                           68   \n",
       "8                     34                            3   \n",
       "9                     30                            3   \n",
       "11                    26                            0   \n",
       "12                    26                           12   \n",
       "14                    21                            1   \n",
       "13                    18                            0   \n",
       "\n",
       "                              Representative Document  \n",
       "3   I'm having trouble finding the course material...  \n",
       "0   6. \"The pizza available in the cantine is terr...  \n",
       "2   The academic workload is becoming overwhelming...  \n",
       "1   There seems to be a scarcity of available inte...  \n",
       "4   10. \"I've had some negative experiences with o...  \n",
       "5   I'm disappointed with the gender inequality in...  \n",
       "6    The financial aid process is so slow, and I n...  \n",
       "7   It's frustrating to have limited access to res...  \n",
       "10  9. \"The housing options near campus are very l...  \n",
       "8    It's frustrating that my health insurance doe...  \n",
       "9   2. \"I've had to commute from the suburbs to th...  \n",
       "11  \"The student affairs office seems to be disorg...  \n",
       "12  I wish the university offered better mental he...  \n",
       "14  2. \"I'm really interested in trying out some n...  \n",
       "13  It's frustrating to have limited access to aca...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# train BERTopic\n",
    "bert_model, topics = train_bertopic(\n",
    "    join_texts, \n",
    "    nr_topics=16, \n",
    "    verbose=False, \n",
    "    min_topic_size=7, \n",
    "    top_n_words=10,\n",
    "    calculate_probabilities=True\n",
    ")\n",
    "\n",
    "topics_per_doc, probs = bert_model.transform(join_texts)  # probs.shape = (num_docs, num_topics)\n",
    "\n",
    "num_docs, num_topics = probs.shape\n",
    "\n",
    "# Creating topic words dictionary\n",
    "topic_words_dict = {}\n",
    "for topic_id, words in bert_model.get_topics().items():\n",
    "    if topic_id == -1:\n",
    "        continue  # outliers\n",
    "    topic_words_dict[topic_id] = [w for w, _ in words]\n",
    "\n",
    "# Topic ids with max probability\n",
    "top1_idx = np.argmax(probs, axis=1)\n",
    "\n",
    "# Topic id's with second max probability\n",
    "probs_copy = probs.copy()\n",
    "for i in range(num_docs):\n",
    "    probs_copy[i, top1_idx[i]] = -1  # replace max probability with -1\n",
    "top2_idx = np.argmax(probs_copy, axis=1)\n",
    "\n",
    "# Count number of complaints for each topic (first and second max)\n",
    "top1_counts = Counter(top1_idx)\n",
    "top2_counts = Counter(top2_idx)\n",
    "\n",
    "# Searcing for representative original document - the one with max probability of the top probability topic\n",
    "representative_idx = {}   \n",
    "\n",
    "for topic_id in topic_words_dict.keys():\n",
    "    docs_idx = np.where(top1_idx == topic_id)[0]\n",
    "    if len(docs_idx) == 0:\n",
    "        representative_idx[topic_id] = None\n",
    "    else:\n",
    "        doc_probs = probs[docs_idx, topic_id]\n",
    "        rep_idx = docs_idx[np.argmax(doc_probs)]\n",
    "        representative_idx[topic_id] = rep_idx\n",
    "\n",
    "# Final table\n",
    "rows = []\n",
    "for topic_id in topic_words_dict.keys():\n",
    "    idx = representative_idx.get(topic_id)\n",
    "    orig_text = texts[idx] if idx is not None else \"\"\n",
    "    rows.append({\n",
    "        'Topic': topic_id,\n",
    "        'Top Words': \", \".join(topic_words_dict[topic_id]),\n",
    "        'Documents (max prob)': top1_counts.get(topic_id, 0),\n",
    "        'Documents (second max prob)': top2_counts.get(topic_id, 0),\n",
    "        'Representative Document': orig_text\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows).sort_values('Topic').reset_index(drop=True)\n",
    "df.sort_values(by=\"Documents (max prob)\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ozon_nlp)",
   "language": "python",
   "name": "ozon_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
